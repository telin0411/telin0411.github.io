<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="./Te-Lin Wu_files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WYV42QGLZ8');
  </script>

  
  <meta name="author" content="Te-Lin Wu">    
  <meta name="viewport" content="width=device-width, initial-scale=1">    
  <link rel="shortcut icon" href="https://www.ucla.edu/favicon.ico">
  <meta name="description" content="Te-Lin Wu is a CS PhD student at UCLA.">
  <meta name="keywords" content="Te-Lin,Te-Lin Wu,Multimodal,Natural Language Processing,NLP,Computer,Vision,Machine,Learning,UCLA">
  <title>Te-Lin Wu</title>

  <link rel="stylesheet" href="./Te-Lin Wu_files/font.css">
  <link rel="stylesheet" href="./Te-Lin Wu_files/main.css">
  <script src="./Te-Lin Wu_files/main.js"></script>
  <script src="./Te-Lin Wu_files/scroll.js"></script>
  <!-- Github; Place this tag in your head or just before your close body tag. -->
  <script async="" defer="" src="./Te-Lin Wu_files/buttons.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">

    <div class="outercontainer"> 
      <script src="./Te-Lin Wu_files/header.js"></script><header><div class="container header"> <div class="ftheader text"></header>
      <header>
        <div class="container header">
          <div class="ftheader text"><a href="#home">Home</a></div>
          <div class="ftheader text"><a href="#publications">Publications</a></div>
          <div class="ftheader text"><a href="https://drive.google.com/file/d/1VuRrIQuc9oWoo2NnoSM88QQJXsrcQLxL/view?usp=sharing">CV (as of April 2024)</a></div>
        </div>
      </header>
      <div class="container body">

        <div class="content heading anchor" id="home">
          <div class="text info">
            <h1>Te-Lin Wu (吳德霖)</h1>
            <p>
            </p>
            <!-- <div>PhD Student</div> -->
            <!-- <div>Research Engineer</div> -->
            <div>Member of Technical Staff, AI</div>
            <div></div>
            <!-- <div>University of California, Los Angeles</div> -->
            <!-- <div><a href="https://character.ai/">Character.ai</a></div> -->
            <div><a href="">Stealth Startup in Video AI</a></div>
            <div>Email:&nbsp;telinwu [at] cs (dot) ucla (dot) edu</div>
            <p>
            <span><a href="https://scholar.google.com/citations?user=Q5aezXQAAAAJ&hl=en">Google Scholar</a></span> / 
            <span><a href="https://www.linkedin.com/in/telinwu">LinkedIn</a></span> / 
            <span><a href="https://github.com/telin0411">Github</a></span>
            </p>
            <p>
            </p>
          </div>
          <div class="img"><img class="avatar" src="./Te-Lin Wu_files/me6.png" alt="Photo"></div>
          <div class="text info">
            <p>
              I am one of the early founding members in the AI domain of a stealth video AI startup.
              Previously, I was a researcher at <a href="https://character.ai/">Character.ai</a> working on large language models (LLMs) and model post-training for smarter and more amusing chatbots.
            <br>
              Prior to that, I obtained my PhD at UCLA <a href="https://vnpeng.net/group/">PlusLab</a> advised by <a href="https://vnpeng.net/">Nanyun (Violet) Peng</a>, 
               where my research focuses on multimodal models across NLP and computer vision.
            <br>
            I have also worked with <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a> on reinforcement learning and vision for robotics topics.
            <br>
            Prior to PhD, I obtained my M.S. at Stanford University where I was advised by <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>,
            and I did my undergrad at National Tsing-Hua University (國立清華大學).
            <br>
            <br>
            Over the summers, I've been lucky to work as research interns in several wonderful groups, including
            <u>Google Research</u>, <u>Meta Reality Labs</u>, <u>Amazon AI</u>, and <u>Adobe Research</u>.
            </p>
          </div>
        </div>

        <div class="content anchor" id="publications">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Selected Publications
            </h3>
            For a full list of my publications, please see <a href="https://scholar.google.com/citations?user=Q5aezXQAAAAJ&hl=en">here</a>. (* denotes equal contributions.)
          </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/docedit.png" alt="colm24_docedit"></div>
              <div class="text">
                <div class="title">Agent-DocEdit: Language-Instructed LLM Agent for Content-Rich Document Editing</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Rajiv Jain</span>,
                  <span class="author">Yufan Zhou</span>,
                  <span class="author">Puneet Mathur</span>,
                  <span class="author">Vlad I Morariu</span>
                </div>
                <div>
                  <span class="venue">COLM 2024</span> /
                  <span class="tag"><a href="">Paper</a></span>
                </div>
                <br>
                <div>
                    A modularized LLM-agent approach for content-rich multimodal document editing.
                </div>
              </div>
            </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/daco.png" alt="daco"></div>
              <div class="text">
                <div class="title">DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation</div> 
                <div class="authors">
                  <span class="author">Xueqing Wu</span>,
                  <span class="author">Rui Zheng</span>,
                  <span class="author">Jingzhen Sha</span>,
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Hanyu Zhou</span>,
                  <span class="author">Tang Mohan</span>,
                  <span class="author">Kai-Wei Chang</span>,
                  <span class="author">Nanyun Peng</span>,
                  <span class="author">Haoran Huang</span>
                </div>
                <div>
                  <span class="venue">NeurIPS Datasets and Benchmarks Track 2024</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/2403.02528">Paper</a></span>
                </div>
                <br>
                <div>
                    A new dataset for complex data analysis tasks and newly proposed RLHF techniques for the tasks.
                </div>
              </div>
            </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/vdebugger.png" alt="vdebugger"></div>
              <div class="text">
                <div class="title">VDebugger: Harnessing Execution Feedback for Debugging Visual Programs</div> 
                <div class="authors">
                  <span class="author">Xueqing Wu</span>,
                  <span class="author">Zongyu Lin</span>,
                  <span class="author">Songyan Zhao</span>,
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Pan Lu</span>,
                  <span class="author">Nanyun Peng</span>,
                  <span class="author">Kai-Wei Chang</span>
                </div>
                <div>
                  <span class="venue">EMNLP 2024 (Findings)</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2406.13444">Paper</a></span>
                </div>
                <br>
                <div>
                    A debugging tool for visual program generator.
                </div>
              </div>
            </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/legal.png" alt="naccl24_legal"></div>
              <div class="text">
                <div class="title">LegalDiscourse: Interpreting When Laws Apply and To Whom</div> 
                <div class="authors">
                  <span class="author">Alexander Spangher</span>,
                  <span class="author">Zihan Xue</span>,
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Mark Hansen</span>,
                  <span class="author">Jonathan May</span>
                </div>
                <div>
                  <span class="venue">NAACL 2024</span> /
                  <span class="tag"><a href="https://aclanthology.org/2024.naacl-long.472.pdf">Paper</a></span>
                </div>
                <br>
                <div>
                  A novel legal-related dataset that emphasizes on the discourse and the taxonomy of span-and-relation parsing.
                </div>
              </div>
            </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/jarvis.png" alt="emnlp23_jarvis"></div>
              <div class="text">
                <div class="title">Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu*</span>,
                  <span class="author">Yu Zhou*</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">EMNLP 2023</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/2310.15066">Paper</a></span> / 
                  <span class="tag"><a href="">Video</a></span>
                </div>
                <br>
                <div>
                  A novel technique to ground active objects in egocentric vision with LLM-enhanced knowledge.
                </div>
              </div>
            </div>

          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/acquired.png" alt="emnlp23_acquired"></div>
              <div class="text">
                <div class="title">ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu*</span>,
                  <span class="author">Zi-Yi Dou*</span>,
                  <span class="author">Qingyuan Hu*</span>,
                  <span class="author">Yu Hou</span>,
                  <span class="author">Nischal Chandra</span>,
                  <span class="author">Marjorie Freedman</span>,
                  <span class="author">Ralph Weischedel</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">EMNLP 2023</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/2311.01620">Paper</a></span> / 
                  <span class="tag"><a href="">Video</a></span>
                </div>
                <br>
                <div>
                  A novel dataset for understanding counterfactual commonsense reasoning in videos.
                </div>
              </div>
            </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/simmcvr.png" alt="acl23_simmcvr"></div>
              <div class="text">
                <div class="title">SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Satwik Kottur</span>,
                  <span class="author">Andrea Madotto</span>,
                  <span class="author">Mahmoud Azab</span>,
                  <span class="author">Pedro Rodriguez</span>,
                  <span class="author">Babak Damavandi</span>,
                  <span class="author">Nanyun Peng</span>,
                  <span class="author">Seungwhan Moon</span> 
                </div>
                <div>
                  <span class="venue">ACL 2023</span> /
                  <span class="tag"><a href="https://aclanthology.org/2023.acl-long.345.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://underline.io/lecture/76882-simmc-vr-a-task-oriented-multimodal-dialog-dataset-with-situated-and-immersive-vr-streams">Video</a></span>
                </div>
                <br>
                <div>
                  A dataset for situated conversational agent with applications in AR/VR shopping domains.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/condition.png" alt="acl23-condition"></div>
              <div class="text">
                <div class="title">Learning Action Conditions from Instructional Manuals for Instruction Understanding</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Caiqi Zhang</span>,
                  <span class="author">Qingyuan Hu</span>, 
                  <span class="author">Alex Spangher</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">ACL 2023</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2205.12420.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://underline.io/lecture/77172-learning-action-conditions-from-instructional-manuals-for-instruction-understanding">Video</a></span>
                </div>
                <br>
                <div>
                  We learn a model to perform pre- and post-conditon inference to actionables in instructional manuals via a weakly-supervised method.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/character.png" alt="emnlp22-char"></div>
              <div class="text">
                <div class="title">Character-Centric Story Visualization via Visual Planning and Token Alignment</div> 
                <div class="authors">
                  <span class="author">Hong Chen</span>,
                  <span class="author">Rujun Han</span>,
                  <span class="author jw">Te-Lin Wu</span>, 
                  <span class="author">Hideki Nakayama</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">EMNLP 2022</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2210.08465.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/PlusLabNLP/VP-CSV">Code</a></span>
                </div>
                <br>
                <div>
                  A method that utilizes grad-cam to propose plausible character plans for story generation (visualization) task.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/multimodal_sort.png" alt="acl22_sort"></div>
              <div class="text">
                <div class="title">Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Alex Spangher</span>,
                  <span class="author">Pegah Alipoormolabashi</span>,
                  <span class="author">Marjorie Freedman</span>,
                  <span class="author">Ralph Weischedel</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">ACL 2022</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2110.08486.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://underline.io/lecture/49866-understanding-multimodal-procedural-knowledge-by-sequencing-multimodal-instructional-manuals">Video</a></span>
                </div>
                <br>
                <div>
                  We propose several sequence-aware pre-training objectives to equip multimodal models with task-order knowledge.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/hyperexpan.png" alt="emnlp21_hyperexpan"></div>
              <div class="text">
                <div class="title">HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning</div> 
                <div class="authors">
                  <span class="author">Mingyu Ma</span>,
                  <span class="author">Muhao Chen*</span>,
                  <span class="author jw">Te-Lin Wu*</span>, 
                  <span class="author">Nanyun Peng</span>, 
                </div>
                <div>
                  <span class="venue">Findings of EMNLP 2021</span>
                </div>
                <div>
                  <span class="tag"><a href="https://arxiv.org/pdf/2109.10500.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/PlusLabNLP/HyperExpan">Code</a></span>
                </div>
                <br>
                <div>
                  Using a hyperbolic representation learning scheme is more effective more KG taxonomy expansion.
                </div>
              </div>
            </div>
                     
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/com2sense.png" alt="acl21_com2sense"></div>
              <div class="text">
                <div class="title">COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences</div> 
                <div class="authors">
                  <span class="author">Shikhar Singh∗</span>,
                  <span class="author">Nuan Wen∗</span>,
                  <span class="author">Yu Hou</span>, 
                  <span class="author">Pegah Alipoormolabashi</span>,
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Xuezhe Ma</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">Findings of ACL 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2106.00969.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/PlusLabNLP/Com2Sense">Dataset & Codes</a></span> /
                </div>
                <br>
                <div>
                  A dataset for complementary commonsense reasoning collected via a model-in-the-loop gamified session.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/melinda.png" alt="aaai21_melinda"></div>
              <div class="text">
                <div class="title">MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Shikhar Singh</span>,
                  <span class="author">Sayan Paul</span>,
                  <span class="author">Gully Burns</span>,
                  <span class="author">Nanyun Peng</span>
                </div>
                <div>
                  <span class="venue">AAAI 2021</span> /
                  <span class="tag"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17657">Paper</a></span> / 
                  <span class="tag"><a href="https://drive.google.com/drive/folders/1dAzIxfEBN7jKuHXOFAgxr64Bz_szeLcQ?usp=sharing">Dataset & Codes</a></span>
                </div>
                <br>
                <div>
                  A dataset for multimodal biomedical method classification.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/lampret.png" alt="naacl21_lampret"></div>
              <div class="text">
                <div class="title">LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Cheng Li</span>,
                  <span class="author">Mingyang Zhang</span>,
                  <span class="author">Tao Chen</span>,
                  <span class="author">Spurthi Amba Hombaiah</span>,
                  <span class="author">Michael Bendersky</span>
                </div>
                <div>
                  <span class="venue">ViGIL Workshop, NAACL 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2104.08405.pdf">Paper</a></span> 
                </div>
                <br>
                <div>
                  A pre-training paradigm to exploit document layout to learn a document representation.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/pga.png" alt="iclr20_pga"></div>
              <div class="text">
                <div class="title">Program Guided Agent</div> 
                <div class="authors">
                  <span class="author">Shao-Hua Sun</span>,
                  <span class="author jw">Te-Lin Wu</span>,
                  <span class="author">Joseph J. Lim</span>
                </div>
                <div>
                  <span class="venue">ICLR 2020</span>
                </div>
                <div>
                  <span class="tag"><a href="https://openreview.net/pdf?id=BkxUvnEYDH">Paper</a></span> / 
                  <span class="tag"><a href="https://shaohua0116.github.io/ProgramGuidedAgent/">Project Page</a></span>
                </div>
                <br>
                <div>
                  A framework to programmatically control an RL-trained agent.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/demo2vec.png" alt="cvpr18_demo2vec"></div>
              <div class="text">
                <div class="title">Demo2Vec: Reasoning Object Affordances from Online Videos</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu*</span>,
                  <span class="author">Kuan Fang*</span>,
                  <span class="author">Daniel Yang</span>,
                  <span class="author">Silvio Savarese</span>,
                  <span class="author">Joseph J. Lim</span>
                </div>
                <div>
                  <span class="venue">CVPR 2018</span>
                </div>
                <div>
                  <span class="tag"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://sites.google.com/view/demo2vec/">Project Page</a></span>
                </div>
                <br>
                <div>
                  Learning to infer object affordance with a video demonstration of how to interact with objects.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./Te-Lin Wu_files/feedback_net.png" alt="cvpr17_feedback_net"></div>
              <div class="text">
                <div class="title">Feedback Networks</div> 
                <div class="authors">
                  <span class="author jw">Te-Lin Wu*</span>,
                  <span class="author">Amir R. Zamir*</span>,
                  <span class="author">Lin Sun</span>,
                  <span class="author">William B. Shen</span>,
                  <span class="author">Bertram E. Shi</span>,
                  <span class="author">Jitendra Malik</span>,
                  <span class="author">Silvio Savarese</span>
                </div>
                <div>
                  <span class="venue">CVPR 2017</span>
                </div>
                <div>
                  <span class="tag"><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zamir_Feedback_Networks_CVPR_2017_paper.pdf">Paper</a></span> / 
                  <span class="tag"><a href="http://feedbacknet.stanford.edu/">Project Page</a></span>
                </div>
                <br>
                <div>
                  A study of feedback mechanism of convolutional neural networks.
                </div>
              </div>
            </div>

        </div>  <!-- content -->
      </div> <!-- container -->
    </div> <!-- outer container -->
    <script>showPubs(0);</script>
    <script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>

  



</div></body></html>
